{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03896a6-9c7d-4fab-81bd-a0d2c3fcbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Homework 5 Solution Template\n",
    "### CSCI 4270 / 6270\n",
    "### Spring 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c282e2-1ad6-44cd-859b-1184b0f0bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from PIL import Imageq\n",
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "252460fc-a840-4806-9c7d-5a17fcd6399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement and test the utilities in support of evaluating the results\n",
    "from the region-by-region decisions and turning them into detections.\n",
    "\n",
    "All rectangles are four component lists (or tuples) giving the upper\n",
    "left and lower right corners of an axis-aligned rectangle.  For example, \n",
    "[2, 9, 12, 18] has upper left corner (2,9) and lower right (12, 18)\n",
    "\n",
    "The region predictions for an image are stored in a list of dictionaries,\n",
    "each giving the class, the activation and the bounding rectangle.\n",
    "For example,\n",
    "\n",
    "{\n",
    "    \"class\": 2,\n",
    "    \"a\":  0.67,\n",
    "    \"rectangle\": (18, 14, 50, 75)\n",
    "}\n",
    "\n",
    "if the class is 0 this means there is no detection and the rectangle\n",
    "should be ignored.  The region predictions must be turned into the\n",
    "detection results by filtering those with class 0 and through non\n",
    "maximum supression.  The resulting regions should be considered the\n",
    "\"detections\" for the image.\n",
    "\n",
    "After this, detections should be compared to the ground truth \n",
    "\n",
    "The ground truth regions for an image are stored as a list of dictionaries. \n",
    "Each dictionary contains the region's class and bounding rectangle.\n",
    "Here is an example dictionary:\n",
    "\n",
    "{\n",
    "    \"class\":  3,\n",
    "    \"rectangle\": (15, 20, 56, 65)\n",
    "}\n",
    "\n",
    "Class 0 will not appear in the ground truth.  \n",
    "\"\"\"\n",
    "\n",
    "def area(rect):\n",
    "    h = rect[3] - rect[1]\n",
    "    w = rect[2] - rect[0]\n",
    "    return h * w\n",
    "\n",
    "\n",
    "def iou(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Input: two rectangles\n",
    "    Output: IOU value, which should be 0 if the rectangles do not overlap.\n",
    "    \"\"\" \n",
    "    x0, y0, x1, y1 = rect1\n",
    "    u0, v0, u1, v1 = rect2\n",
    "    ir = (max(x0, u0), max(y0, v0), min(x1, u1), min(y1, v1))\n",
    "    if ir[0] >= ir[2] or ir[1] >= ir[3]:\n",
    "        return 0\n",
    "    else:\n",
    "        return area(ir) / (area(rect1) + area(rect2) - area(ir))\n",
    "\n",
    "\n",
    "def predictions_to_detections(predictions, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Input: List of region predictions\n",
    "\n",
    "    Output: List of region predictions that are considered to be\n",
    "    detection results. These are ordered by activation with all class\n",
    "    0 predictions eliminated, and non-maximum suppression\n",
    "    applied.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate(detections, gt_detections, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    1. The detections returned by the predictions_to_detections function\n",
    "    2. The list of ground truth regions, and\n",
    "    3. The IOU threshold\n",
    "\n",
    "    The calculation must compare each detection region to the ground\n",
    "    truth detection regions to determine which are correct and which\n",
    "    are incorrect.  Finally, it must compute the average precision for\n",
    "    up to n detections.\n",
    "\n",
    "    Returns:\n",
    "    list of correct detections,\n",
    "    list of incorrect detections,\n",
    "    list of ground truth regions that are missed,\n",
    "    AP@n value.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01758bd3-d4ee-4956-97e0-800e0946e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou for (0, 5, 11, 15) (2, 9, 12, 18) is 0.37\n",
      "iou for (2, -3, 11, 4) (2, 9, 12, 18) is 0.00\n",
      "iou for (3, 12, 9, 15) (2, 9, 12, 18) is 0.20\n"
     ]
    }
   ],
   "source": [
    "def test_iou():\n",
    "    \"\"\"\n",
    "    Use this function for you own testing of your IOU function\n",
    "    \"\"\"\n",
    "    # should be .370\n",
    "    rect1 = (0, 5, 11, 15)\n",
    "    rect2 = (2, 9, 12, 18)\n",
    "    res = iou(rect1, rect2)\n",
    "    print(f\"iou for {rect1} {rect2} is {res:1.2f}\")\n",
    "\n",
    "    # should be 0\n",
    "    rect1 = (2, -3, 11, 4)\n",
    "    res = iou(rect1, rect2)\n",
    "    print(f\"iou for {rect1} {rect2} is {res:1.2f}\")\n",
    "\n",
    "    # should be 0.2\n",
    "    rect1 = (3, 12, 9, 15)\n",
    "    res = iou(rect1, rect2)\n",
    "    print(f\"iou for {rect1} {rect2} is {res:1.2f}\")\n",
    "\n",
    "test_iou()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849a3d77-2858-407b-a7e4-9337d4621cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation_code(in_json_file):\n",
    "    with open(in_json_file, \"r\") as in_fp:\n",
    "        data = json.load(in_fp)\n",
    "    \n",
    "    region_predictions = data[\"region_predictions\"]\n",
    "    gt_detections = data[\"gt_detections\"]\n",
    "\n",
    "    detections = predictions_to_detections(region_predictions)\n",
    "    print(f\"DETECTIONS: count = {len(detections)}\")\n",
    "    if len(detections) >= 2:\n",
    "        print(f\"DETECTIONS: first activation {detections[0]['a']:.2f}\" )\n",
    "        print(f\"DETECTIONS: last activation {detections[-1]['a']:.2f}\")\n",
    "    elif len(detections) == 1:\n",
    "        print(f\"DETECTIONS: only activation {detections[0]['a']:.2f}\")\n",
    "    else:\n",
    "        print(f\"DETECTIONS: no activations\")\n",
    "\n",
    "    correct, incorrect, missed, ap = evaluate(detections, gt_detections)\n",
    "\n",
    "    print(f\"AP: num correct {len(correct)}\")\n",
    "    if len(correct) > 0:\n",
    "        print(f\"AP: first correct activation {correct[0]['a']:.2f}\")\n",
    "\n",
    "    print(f\"AP: num incorrect {len(incorrect)}\")\n",
    "    if len(incorrect) > 0:\n",
    "        print(f\"AP: first incorrect activation {incorrect[0]['a']:.2f}\")\n",
    "\n",
    "    print(f\"AP: num ground truth missed {len(missed)}\")\n",
    "    print(f\"AP: final AP value {ap:1.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58701693-1bf6-41fa-a939-17b684db888e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTIONS: count = 2\n",
      "DETECTIONS: first activation 0.90\n",
      "DETECTIONS: last activation 0.70\n",
      "AP: num correct 1\n",
      "AP: first correct activation 0.90\n",
      "AP: num incorrect 1\n",
      "AP: first incorrect activation 0.70\n",
      "AP: num ground truth missed 2\n",
      "AP: final AP value 0.364\n"
     ]
    }
   ],
   "source": [
    "test_evaluation_code('eval_test1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09e592-f4ec-47e0-85b0-e26c89a5df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluation_code('eval_test2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf04ca1-27a9-45aa-9223-d5b1b5103ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluation_code('eval_test3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c2d4a-bee7-483c-a33b-785a99ff5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluation_code('eval_test4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d7edc7-531b-42c0-b5d3-95413118731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Skeleton model class. You will have to implement the classification and regression layers,\n",
    "along with the forward method.\n",
    "'''\n",
    "\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RCNN, self).__init__()\n",
    "\n",
    "        # Pretrained backbone. If you are on the cci machine then this will not be able to automatically download\n",
    "        #  the pretrained weights. You will have to download them locally then copy them over.\n",
    "        #  During the local download it should tell you where torch is downloading the weights to, then copy them to \n",
    "        #  ~/.cache/torch/checkpoints/ on the supercomputer.\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Remove the last fc layer of the pretrained network.\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # Freeze backbone weights. \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # TODO: Implement the fully connected layers for classification and regression.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward. Should return a (batch_size x num_classes) tensor for classification\n",
    "        #           and a (batch_size x num_classes x 4) tensor for the bounding box regression. \n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee23751-959c-47ba-912a-f813932baa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Dictionaries mapping class labels to names.\n",
    "LABELS_TO_NAMES = {0: 'nothing',\n",
    "                   1: 'bicycle',\n",
    "                   2: 'car',\n",
    "                   3: 'motorbike',\n",
    "                   4: 'person',}\n",
    "\n",
    "\n",
    "LABELS_TO_NAMES_LARGE = {0: 'nothing',\n",
    "                         1: 'aeroplane',\n",
    "                         2: 'bicycle',\n",
    "                         3: 'bird',\n",
    "                         4: 'boat',\n",
    "                         5: 'bottle',\n",
    "                         6: 'bus',\n",
    "                         7: 'car',\n",
    "                         8: 'cat',\n",
    "                         9: 'chair',\n",
    "                         10: 'cow',\n",
    "                         11: 'diningtable',\n",
    "                         12: 'dog',\n",
    "                         13: 'horse',\n",
    "                         14: 'motorbike',\n",
    "                         15: 'person',\n",
    "                         16: 'pottedplant',\n",
    "                         17: 'sheep',\n",
    "                         18: 'sofa',\n",
    "                         19: 'train',\n",
    "                         20: 'tvmonitor'}\n",
    "\n",
    "\n",
    "class HW5Dataset(Dataset):\n",
    "    '''\n",
    "    Dataset for Train and Validation.\n",
    "    Input:\n",
    "        data_root - path to either the train or valid image directories\n",
    "        json_file - path to either train.json or valid.json\n",
    "    Output:\n",
    "        candidate_region - 3 x M x M tensor\n",
    "        ground_truth_bbox - 1 x 4 tensor\n",
    "        ground_truth_class\n",
    "    '''\n",
    "    def __init__(self, data_root, json_file, candidate_region_size=224):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.candidate_region_size = candidate_region_size\n",
    "\n",
    "        self.images = []\n",
    "        self.candidate_bboxes = torch.empty((0, 4), dtype=int)\n",
    "        self.ground_truth_bboxes = torch.empty((0, 4), dtype=int)\n",
    "        self.ground_truth_classes = torch.empty(0, dtype=int)\n",
    "        for key, values in data_dict.items():\n",
    "            for val in values:\n",
    "                self.images.append(key)\n",
    "                self.candidate_bboxes = torch.cat((self.candidate_bboxes, torch.tensor(val['bbox']).unsqueeze(0)))\n",
    "                self.ground_truth_bboxes = torch.cat((self.ground_truth_bboxes, torch.tensor(val['gt_bbox']).unsqueeze(0)))\n",
    "                self.ground_truth_classes = torch.cat((self.ground_truth_classes, torch.tensor(val['class']).unsqueeze(0)))\n",
    "\n",
    "        # Transform to convert to tensor, resize, and normalize.\n",
    "        self.transform = transforms.Compose([transforms.Resize((candidate_region_size, candidate_region_size)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image.\n",
    "        image_path = join(self.data_root, self.images[idx])\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Crop the image to the candidate region.\n",
    "        candidate_bbox = self.candidate_bboxes[idx, :]\n",
    "        candidate_region = image.crop((candidate_bbox[0].item(), candidate_bbox[1].item(), candidate_bbox[2].item(), candidate_bbox[3].item()))\n",
    "        \n",
    "        width, height = candidate_region.size\n",
    "        x_scale = self.candidate_region_size / width\n",
    "        y_scale = self.candidate_region_size / height\n",
    "\n",
    "        # Transform to resize, convert to tensor, and normalize.\n",
    "        candidate_region = self.transform(candidate_region)\n",
    "        \n",
    "        # Resize ground truth bounding box.\n",
    "        gt_bbox = self.ground_truth_bboxes[idx, :]\n",
    "        resized_gt_x0 = (gt_bbox[0] - candidate_bbox[0]) * x_scale / self.candidate_region_size\n",
    "        resized_gt_y0 = (gt_bbox[1] - candidate_bbox[1]) * y_scale / self.candidate_region_size\n",
    "        resized_gt_x1 = (gt_bbox[2] - candidate_bbox[0]) * x_scale / self.candidate_region_size\n",
    "        resized_gt_y1 = (gt_bbox[3] - candidate_bbox[1]) * y_scale / self.candidate_region_size\n",
    "        \n",
    "        resized_gt_bbox = torch.tensor([resized_gt_x0, resized_gt_y0, resized_gt_x1, resized_gt_y1])\n",
    "        \n",
    "        return candidate_region, resized_gt_bbox, self.ground_truth_classes[idx]\n",
    "\n",
    "\n",
    "class HW5DatasetTest(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Test.\n",
    "    Input:\n",
    "        data_root - path to the test image directory\n",
    "        json_file - path to test.json\n",
    "    Returns:\n",
    "        image - numpy array A x B x 3 (RGB)\n",
    "        candidate_regions - NUM_CANDIDATE_REGIONS x 3 x M x M tensor\n",
    "        candidate_bboxes - all candidate bounding boxes for an image \n",
    "        ground_truth_bboxes - all ground truth bounding boxes for an image\n",
    "        ground_truth_classes - all ground truth classes for an image\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, json_file, candidate_region_size=224):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "\n",
    "        self.data_root = data_root\n",
    "\n",
    "        self.images = []\n",
    "        self.candidate_bboxes = []\n",
    "        self.ground_truth_bboxes = []\n",
    "        self.ground_truth_classes = []\n",
    "        for key, values in data_dict.items():\n",
    "            self.images.append(key)\n",
    "\n",
    "            bboxes = torch.empty((len(values['candidate_bboxes']), 4), dtype=int)\n",
    "            for i, bbox in enumerate(values['candidate_bboxes']):\n",
    "                bboxes[i, :] = torch.tensor(bbox)\n",
    "            self.candidate_bboxes.append(bboxes)\n",
    "\n",
    "            labels = torch.empty((len(values['gt_bboxes'])), dtype=int)\n",
    "            bboxes = torch.empty((len(values['gt_bboxes']), 4), dtype=int)\n",
    "            for i, bbox in enumerate(values['gt_bboxes']):\n",
    "                bboxes[i, :] = torch.tensor(bbox['bbox'])\n",
    "                labels[i] = bbox['class']\n",
    "            self.ground_truth_bboxes.append(bboxes)\n",
    "            self.ground_truth_classes.append(labels)\n",
    "\n",
    "        self.candidate_region_size = candidate_region_size\n",
    "\n",
    "        # Transform to resize, convert to tensor, and normalize.\n",
    "        self.transform = transforms.Compose([transforms.Resize((candidate_region_size, candidate_region_size)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image.\n",
    "        image_path = join(self.data_root, self.images[idx])\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Apply transform to resize and normalize the candidate images.\n",
    "        idx_candidate_bboxes = self.candidate_bboxes[idx]\n",
    "        candidate_regions = torch.empty((len(idx_candidate_bboxes), 3, self.candidate_region_size, self.candidate_region_size))\n",
    "        for i, bbox in enumerate(idx_candidate_bboxes):\n",
    "            candidate_region = image.crop((bbox[0].item(), bbox[1].item(), bbox[2].item(), bbox[3].item()))\n",
    "            candidate_region = self.transform(candidate_region)\n",
    "            candidate_regions[i] = candidate_region\n",
    "\n",
    "        return np.array(image), candidate_regions, self.candidate_bboxes[idx], self.ground_truth_bboxes[idx], self.ground_truth_classes[idx]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
